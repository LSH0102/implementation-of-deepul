import copy
import math
from transformer import ABS_RoPE,Embedding,Transformer_Block
from einops import einsum,rearrange
import numpy as np
import torch 
from torch import nn
from deepul.hw1_helper import (
    # Q1
    visualize_q1_data,
    q1_sample_data_1,
    q1_sample_data_2,
    q1_save_results,
    # Q2
    q2a_save_results,
    q2b_save_results,
    visualize_q2a_data,
    visualize_q2b_data,
    # Q3
    q3ab_save_results,
    q3c_save_results,
    # Q4
    q4a_save_results,
    q4b_save_results,
    # Q5
    visualize_q5_data,
    q5a_save_results,
    # Q6
    visualize_q6_data,
    q6a_save_results,
)

device='cuda'

def _get_clones(module, N):
    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])


class iGPT(nn.Module):
    def __init__(self,d_model,vocab_size, H,W,context_length,num_layers,num_heads,d_ff,theta,max_seq_len):
        super().__init__()
        
        self.H=H
        self.W=W
        
        if max_seq_len==None:
            max_seq_len=context_length

        self.pe=ABS_RoPE(theta, d_model, max_seq_len).to(device)
        self.embedding=Embedding(vocab_size, d_model).to(device)
        
        Layer=Transformer_Block(d_model, num_heads, d_ff).to(device)
        
        self.Trans=_get_clones(Layer,num_layers)

        self.Linear=torch.nn.Linear(d_model, vocab_size-1)
        
        self.lm=torch.nn.LayerNorm(vocab_size-1).to(device)
        self.layers=num_layers
        self.d_model=d_model
        
        x=np.zeros((vocab_size-1,3))
        y=np.zeros((4,4,4,1))
        for i in range(0,4):
            for j in range(0,4):
                for k in range(0,4):
                    x[16*i+4*j+k]=[i,j,k]
                    y[i,j,k]=16*i+4*j+k
        self.num2pix=x
        self.pix2num=y
        
        self.softmax=torch.nn.Softmax(dim=-1)
    def preprocess(self,data:np.ndarray):
        batch_size,H,W,C=data.shape 
        x=np.zeros((batch_size,H,W,1))
        x[:,:,:]=data[:,:,:,[0]]*16+4*data[:,:,:,[1]]+data[:,:,:,[2]]
        x=x.astype(np.int16)
        return x
    
    def afterprocess(self,data:np.ndarray):
        '形如(batch_size,H,W,1)的数组, 转化成(batch_size,H,W,3)'
        batch_size,H,W,C=data.shape 
        vocab_size=self.num2pix.shape[0]
        x=np.broadcast_to(self.num2pix,shape=(batch_size,H,W,vocab_size,3))
        x=torch.Tensor(x).to(device)
        data=torch.LongTensor(data).to(device).unsqueeze(-1)
        data=data.broadcast_to((batch_size,H,W,1,3))
        
        out=torch.gather(x, dim=-2, index=data).squeeze(-2)
        
        return out.cpu().detach().numpy()
    
    def forward(self,x:np.ndarray):
        
        x=torch.LongTensor(x).squeeze(-1).to(device)
        x=x.to(device)
        x=self.embedding(x)
        x=self.pe(x)
        for i in range(0,self.layers):
            x=self.Trans[i](x)
    
        x=self.Linear(x)
        x=self.lm(x)
        x=self.softmax(x)
        
        return x
    
    def compute_maxlikelihood(self,x:np.ndarray):
        prob=self.forward(x[:,:-1,:]).to(device)
        
        y=torch.LongTensor(x[:,1:,:]).to(dtype=torch.int64).to(device)
        
        p=torch.gather(prob, dim=-1, index=y).squeeze()
        L=torch.log(p)
        return L.mean()
    
    def sample(self,num_samples):
        with torch.no_grad():
            h=64*np.ones(shape=(num_samples,1,1))
            
            for i in range(0,self.H):
                for j in range(0,self.W):
                    probs=self.forward(h)
                    
                    dist=torch.distributions.Categorical(probs=probs)
                    next_pixcel=dist.sample().unsqueeze(-1).cpu().detach().numpy()[:,[-1],:]
                    h=np.concatenate((h, next_pixcel),axis=1)
                    
            sample=h[:,1:,:]
            
            sample=sample.reshape((num_samples,self.W,self.H,-1))
        return self.afterprocess(sample)
        
        
def q3_b(train_data,test_data,image_shape,dset_id):
    total=train_data.shape[0]
    H,W,C=image_shape
    model=iGPT(d_model=128,vocab_size=65,H=H,W=W,context_length=1+H*W,num_layers=2,
               num_heads=4,d_ff=128,theta=10000,max_seq_len=1+H*W).to(device)
    
    data=model.preprocess(train_data)
    
    data=data.reshape((total,H*W,1))
    
    model.load_state_dict(torch.load('iGPT_shape_3b_2.pth',map_location=device))
    
    batch_size=32
    n_batches=total//batch_size
    epoch=2
    
    opt=torch.optim.Adam(model.parameters(),lr=1e-3)
    
    train_losses=[]
    for i in range(0,epoch):
        rand_indices = np.random.randint(0,total, size=(batch_size*n_batches,))
        for j in range(0,n_batches):
            in_data=data[rand_indices[j*batch_size:(j+1)*batch_size]]
            h=64*np.ones(shape=(batch_size,1,1))
            
            in_data=np.concatenate((h,in_data),axis=1)
            
            L=-model.compute_maxlikelihood(in_data)
            
            print(i,L)
            opt.zero_grad()
            L.backward()
            opt.step()
            
            train_losses.append(L.cpu().detach().numpy())
        torch.save(model.state_dict(),'iGPT_shape_3b_2.pth')
    sample=model.sample(100)
    train_losses=np.array(train_losses)
    return train_losses,np.array([1]),sample
q3ab_save_results(2, 'b', q3_b)

