import copy
import torch
import torch.nn as nn
from torch.nn.utils import spectral_norm
from torch.optim.lr_scheduler import LambdaLR
from transformer import ABS_RoPE,Embedding,Transformer_Block
from deepul.hw3_utils.lpips import LPIPS
from deepul.hw3_helper import *
import deepul.pytorch_util as ptu
import warnings
warnings.filterwarnings('ignore')
device='cuda'

def _get_clones(module, N):
    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])

class iGPT(nn.Module):
    def __init__(self,d_model,vocab_size, H,W,context_length,num_layers,num_heads,d_ff,theta,max_seq_len):
        super().__init__()
        
        self.H=H
        self.W=W
        
        if max_seq_len==None:
            max_seq_len=context_length

        self.pe=ABS_RoPE(theta, d_model, max_seq_len).to(device)
        self.embedding=Embedding(vocab_size, d_model).to(device)
        
        Layer=Transformer_Block(d_model, num_heads, d_ff).to(device)
        
        self.Trans=_get_clones(Layer,num_layers)

        self.Linear=torch.nn.Linear(d_model, vocab_size-1)
        
        self.lm=torch.nn.LayerNorm(vocab_size-1).to(device)
        self.layers=num_layers
        self.d_model=d_model
        
        self.vocab_size=vocab_size
        
        self.softmax=torch.nn.Softmax(dim=-1)
    
    
    def forward(self,x:np.ndarray):
        
        x=torch.LongTensor(x).to(device)
        x=x.to(device)
        x=self.embedding(x)
        x=self.pe(x)
        for i in range(0,self.layers):
            x=self.Trans[i](x)
    
        x=self.Linear(x)
        x=self.lm(x)
        x=self.softmax(x)
        
        return x
    
    def compute_maxlikelihood(self,x:np.ndarray):
        prob=self.forward(x[:,:-1]).to(device)
        
        y=torch.LongTensor(x[:,1:]).to(dtype=torch.int64).to(device)
        y=y.unsqueeze(-1)
        p=torch.gather(prob, dim=-1, index=y).squeeze()
        L=torch.log(p)
        return L.mean()
    
    def sample(self,num_samples):
        with torch.no_grad():
            h=(self.vocab_size-1)*np.ones(shape=(num_samples,1))
            
            for i in range(0,self.H):
                for j in range(0,self.W):
                    probs=self.forward(h)
                    
                    dist=torch.distributions.Categorical(probs=probs)
                    next_pixcel=dist.sample().cpu().detach().numpy()[:,[-1]]
                    h=np.concatenate((h, next_pixcel),axis=1)
                    
            sample=h[:,1:]
            
            sample=sample.reshape((num_samples,self.H,self.W))
        return sample
    
class Space2Depth(nn.Module):
    def __init__(self,block_size:int):
        super().__init__()
        self.block_size=block_size
        self.block_size_sq=block_size*block_size
    def forward(self,x:torch.Tensor):
        out=x.permute(0,2,3,1)
        batch_size,s_height,s_width,s_depth=out.shape
        d_depth=s_depth*self.block_size_sq
        d_width=int(s_width/self.block_size)
        d_height=int(s_height/self.block_size)
        t_1=out.split(self.block_size,2)
        stack=[ t_t.reshape((batch_size,d_height,d_depth)) for t_t in t_1]
        out=torch.stack(stack,1)
        out=out.permute(0,2,1,3)
        out=out.permute(0,3,1,2)
        return out
class Downsample_Conv2d(nn.Module):
    def __init__(self,in_dim,out_dim,kernel_size=(3,3),stride=1,padding=1):
        super().__init__()
        self.Sp2Dep=Space2Depth(block_size=2)
        self.conv=spectral_norm(torch.nn.Conv2d(in_channels=in_dim, 
                                                out_channels=out_dim, kernel_size=kernel_size,stride=stride,padding=padding),)
    def forward(self,x:torch.Tensor):
        x=self.Sp2Dep(x)
        x1,x2,x3,x4=x.chunk(4,dim=1)
        x=(x1+x2+x3+x4)/4.0
        x=self.conv(x)
        return x
class ResnetBlockDown(nn.Module):
    def __init__(self,in_dim:int, kernel_size=(3,3), n_filters=256):
        super().__init__()
        self.acti=torch.nn.ReLU()
        self.conv=spectral_norm(torch.nn.Conv2d(in_channels=in_dim, out_channels=n_filters, 
                                                kernel_size=kernel_size,padding=1).to(device),)
        self.D1=Downsample_Conv2d(n_filters, n_filters,kernel_size,padding=1).to(device)
        self.D2=Downsample_Conv2d(in_dim, n_filters,kernel_size=(1,1),padding=0).to(device)
    def forward(self,x:torch.Tensor):
        y=x
        y=self.conv(y)
        y=self.acti(y)
        residual=self.D1(y)
        shortcut=self.D2(x)
        return residual+shortcut
class ResBlock(nn.Module):
    def __init__(self,n_filters=128):
        super().__init__()
        self.conv1=spectral_norm(nn.Conv2d(in_channels=n_filters, out_channels=n_filters, kernel_size=(3,3),padding=1).to(device),)
        self.acti=nn.ReLU()
        self.conv2=spectral_norm(nn.Conv2d(in_channels=n_filters, out_channels=n_filters, kernel_size=(3,3),padding=1).to(device),)
    def forward(self,x:torch.Tensor):
        x=self.acti(x)
        x=self.conv1(x)
        x=self.acti(x)
        x=self.conv2(x)
        return x

class Discriminator(nn.Module):
    def __init__(self,n_filters=128):
        super().__init__()
        self.RD1=ResnetBlockDown(3,n_filters=n_filters).to(device)
        self.RD2=ResnetBlockDown(128,n_filters=n_filters).to(device)
        self.Rs1=ResBlock(n_filters)
        self.Rs2=ResBlock(n_filters)
        self.acti=nn.LeakyReLU(negative_slope=0.2)
        self.L=spectral_norm(nn.Linear(n_filters, 1),)
        self.sig=torch.nn.Sigmoid()
        
    def forward(self,x:torch.Tensor):
        x=self.RD1(x)
        x=self.RD2(x)
        x=self.Rs1(x)
        x=self.Rs2(x)
        x=self.acti(x)
        x=torch.sum(x,dim=[-1,-2])
        x=self.L(x)
        x=0.03+0.95*self.sig(x)
        return x

class Vit(nn.Module):
    def __init__(self,d_model,D,Patches, Hp,Wp,num_layers,num_heads,d_ff,theta):
        super().__init__()
        self.patches=Patches
        self.Hp=Hp
        self.Wp=Wp
        self.D=D
        
        self.pe=ABS_RoPE(theta=theta, d_k=d_model, max_seq_len=self.patches*self.patches)
        self.patch_emb=torch.nn.Linear(self.Hp*self.Wp*3 ,d_model ) #把每个patch投影到 d_model
        Layer=Transformer_Block(d_model, num_heads, d_ff)
        
        self.Trans=_get_clones(Layer,num_layers)
        self.lm=torch.nn.LayerNorm(d_model)
        self.linear=torch.nn.Linear(d_model, self.D) #d_model投影到latent space 的向量长度D
        
        self.d_model=d_model
        self.layers=num_layers
        
        
    def forward(self,x:torch.Tensor):
        batch_size=x.shape[0]
        patch=[]
        y=x.split(self.Hp,dim=2)
        for y1 in y:
            for item in y1.split(self.Wp,dim=3):
                patch.append(item)
        #现在有64个(batch_size,3,4,4)的patch
        xp=torch.stack([self.patch_emb (item.flatten(start_dim=1)) for item in patch],dim=1)  #(batch_size,patches,d_model)
        
        x=self.pe(xp)
        for i in range(0,self.layers):
            x=self.Trans[i](x)
        x=self.lm(x)
        x=self.linear(x)        #返回(batch_size, patches,D=256)的向量
        return x
    
class Encoder(nn.Module):
    def __init__(self,d_model,D,Patches, Hp,Wp,num_layers,num_heads,d_ff,theta):
        super().__init__()
        self.vit=Vit(d_model, D, Patches, Hp, Wp, num_layers, num_heads, d_ff, theta).to(device)
        
    def forward(self,x:torch.Tensor):
        return self.vit(x)
    
class Decoder(nn.Module):
    def __init__(self,d_model, P,Hp,Wp,num_layers,num_heads,d_ff,theta):
        super().__init__()
        #对一个Decoder 序列是(batch_size,patches,D=256) 把每个D映射为8 x 8的图像块
        self.H=Hp
        self.W=Wp
        self.patch=P
        self.pe=ABS_RoPE(theta, d_k=d_model, max_seq_len=self.patch*self.patch)
        Layer=Transformer_Block(d_model, num_heads, d_ff)
        
        self.Trans=_get_clones(Layer,num_layers)
        self.lm=torch.nn.LayerNorm(d_model)
        self.linear=torch.nn.Linear(d_model, self.H*self.W*3)  
        self.layers=num_layers
        
    def forward(self,x:torch.Tensor):
        batch_size=x.shape[0]
        x=self.pe(x)
        for i in range(0,self.layers):
            x=self.Trans[i](x)
        x=self.lm(x)
        x=self.linear(x)
        x=x.reshape((batch_size,self.patch*self.patch,3,self.H,self.W))
        patch=x.split(1,dim=1)
        e=[]
        for j in range(0,self.patch):
            e.append(torch.concat(patch[j*self.patch:(j+1)*self.patch],dim=-1))
        x=torch.concat(e,dim=-2)
        x=x.squeeze()
        return x
        
        
class VQVAE(nn.Module):
    def __init__(self,d_model:int,H,W,K,D,beta=0.2,gamma=0.99):
        super().__init__()
        self.d_model=d_model
        self.H=H
        self.W=W
        self.K=K
        self.D=D
        t=torch.empty((K,D),device=device)
        self.codebook=torch.nn.init.uniform_(t,-1,1)
        self.encoder=Encoder(d_model=256, D=D, Patches=8, Hp=4, Wp=4, num_layers=4, num_heads=8, d_ff=256, theta=10000).to(device)
        self.decoder=Decoder(d_model=256, P=8, Hp=4, Wp=4, num_layers=4, num_heads=8, d_ff=256, theta=10000) .to(device)
        
        self.recon_fn=torch.nn.MSELoss()
        self.beta=beta
        self.gamma=0.99
        self.N=torch.zeros((K,),device=device)
        self.m=torch.zeros((self.K,self.D),device=device)
        self.prior=iGPT(d_model=128, vocab_size=self.K+1, H=self.H, W=self.W, 
                        context_length=self.H*self.W, num_layers=4, num_heads=4, d_ff=128, theta=10000, max_seq_len=self.H*self.W)
    def compute_ek(self,x:np.ndarray):
        x=torch.Tensor(x).to(device)
        Ex=self.encoder(x)              #(batch_size,64,256)
        batch_size=x.shape[0]
        #Ex=Ex.reshape((batch_size,self.dim,self.H*self.W)).permute(0,2,1)  #(batch_size,64,256)
        Ex=Ex.reshape((batch_size*64,self.D))
        
        dist=torch.cdist(Ex,self.codebook)
        indices=torch.argmin(dist,dim=-1)   #抽出和每个z最近的索引
        
        
        return Ex,indices
        
    def VAE_LOSS(self,x:np.ndarray):
        batch_size=x.shape[0]
        Ze,indices=self.compute_ek(x)
        indices=indices.unsqueeze(1)
        prior_data=indices.reshape((batch_size,64)).cpu().detach().numpy()
        ind2=indices
        indices=indices.broadcast_to((indices.shape[0],256)).to(torch.int64)
        Zq=torch.gather(self.codebook,dim=0,index=indices)
        
        feature=(Zq-Ze).detach()+Ze
        feature=feature.reshape((batch_size,64,256))
        #feature=feature.permute(0,2,1)
        #feature=feature.reshape((batch_size,256,self.H,self.W))
        
        Dx=self.decoder(feature)             #(batch_size,3,32,32)
        x=torch.Tensor(x).to(device)
        
        recon_loss=self.recon_fn(Dx,x)
        
        Ze=Ze.reshape((batch_size,64,256))
        ek=Zq.reshape((batch_size,64,256))
        sgZe=Ze.detach()
        VQ=self.recon_fn(sgZe,ek)
        
        sgek=ek.detach()
        comm_loss=self.recon_fn(Ze,sgek)
        
        
        #然后更新e_k
        with torch.no_grad():
            ind=torch.zeros((indices.shape[0],self.codebook.shape[0]),device=device)
            ind=ind.scatter(1, ind2, 1)
            
            self.N=self.gamma*self.N+(1-self.gamma)*ind.sum(dim=0)
            self.m=self.gamma*self.m+(1-self.gamma)*torch.matmul(ind.T,Ze.reshape((batch_size*64,256)))
            deno=self.N.sum()
            self.N=(self.N+1e-5)*deno/(deno+self.K*1e-5)
            self.codebook=self.m/self.N.unsqueeze(1)
        return recon_loss+VQ+self.beta*comm_loss,prior_data, Dx
    
    def recon(self,x:np.ndarray):
        with torch.no_grad():
            batch_size=x.shape[0]
            Ze,indices=self.compute_ek(x)
            x=torch.Tensor(x).to(device)
            
            indices=indices.unsqueeze(1)
            indices=indices.broadcast_to((indices.shape[0],256)).to(torch.int64)
            Zq=torch.gather(self.codebook,dim=0,index=indices)
            
            feature=Zq
            feature=feature.reshape((batch_size,64,256))
            #feature=feature.permute(0,2,1)
            #feature=feature.reshape((batch_size,256,self.H,self.W))
            
            Dx=self.decoder(feature)
        return Dx.cpu().detach().numpy()
    
    def sample(self,num_samples):
        sample=self.prior.sample(num_samples)  #采样出(100,8,8)个token
        sample=sample.reshape((6400,1))
        sample=torch.LongTensor(sample).to(device)
        sample=sample.broadcast_to((6400,self.D))
        zq=torch.gather(self.codebook,dim=0,index=sample)
        zq=zq.reshape((100,64,256)).permute((0,2,1))
        zq=zq.reshape((100,256,8,8))
        x=self.decoder(zq)
        return x.cpu().detach().numpy()
        
        
class ViT_VQGAN(nn.Module):
    def __init__(self,d_model:int,H,W,K,D,beta=0.2,gamma=0.99):
        super().__init__()
        self.VQVAE=VQVAE(d_model=256, H=8, W=8, K=1024, D=256).to(device)
        self.D=Discriminator().to(device)
        self.l1=torch.nn.L1Loss()
        
    def compute_loss(self,x:np.ndarray):
        Lvae,TF_prior,recon=self.VQVAE.VAE_LOSS(x)
        x=torch.Tensor(x).to(device)
        
        Ldis=torch.log(self.D(x))+torch.log(1-self.D(recon))
        Ldis=Ldis.mean()
        Lgen=-torch.log(self.D(recon))
        Lgen=Lgen.mean()
        
        L1=self.l1(recon,x)
        
        return Lvae,Ldis,Lgen,L1, TF_prior

def q3b(train_data, val_data, reconstruct_data):
    total=train_data.shape[0]
    batch_size=256
    
    n_batches=total//batch_size
    
    model=ViT_VQGAN(d_model=256, H=8, W=8, K=1024, D=256)
    model.to(device)
    
    model.load_state_dict(torch.load('q3b.pth'))
    
    opt1=torch.optim.Adam(model.VQVAE.parameters(),lr=1e-3,betas=(0.5,0.9),weight_decay=0)
    opt2=torch.optim.Adam(model.VQVAE.prior.parameters(),lr=1e-3,betas=(0.5,0.9),weight_decay=0)
    opt3=torch.optim.Adam(model.D.parameters(),lr=1e-3,betas=(0.5,0.9),weight_decay=0)
    
    epoch=1
    dis_loss=[]
    recon_train=[]
    recon_test=[]
    for i in range(0,epoch):
        rand=np.random.randint(0,total,size=(batch_size*n_batches,))
        for j in range(0,n_batches):
            data=train_data[rand[j*batch_size:(j+1)*batch_size]]
            Lvae,Ldis,Lgen,L1,prior_data=model.compute_loss(data)
            
            dis_loss.append(Ldis.item())
            
            recon_train.append(Lvae.item())
            L=-Ldis
            opt3.zero_grad()
            L.backward()
            opt3.step()
            print(i,Lvae.item(),Ldis.item())
            
            Lvae,Ldis,Lgen,L1,prior_data=model.compute_loss(data)
            opt1.zero_grad()
            L=Lvae+0.1*L1+0.1*Lgen
            L.backward()
            opt1.step()
            #h=1024*np.ones(shape=(batch_size,1))
            #data=np.concatenate( (h, prior_data),axis=1)
            #L=-model.VQVAE.prior.compute_maxlikelihood(data)
            #opt2.zero_grad()
            #L.backward()
            #opt2.step()
            
            
        with torch.no_grad():
            data=val_data[:12]
            
            Lvae,Ldis,Lgen,L1,prior_data=model.compute_loss(data)
            recon_test.append(Lvae.item())
        torch.save(model.state_dict(),'q3b.pth')
    dis_loss=np.array(dis_loss)
    
    recon_train=np.array(recon_train)
    recon_test=np.array(recon_test)
    data=reconstruct_data
    recon=model.VQVAE.recon(data).transpose(0,2,3,1)
    
    return dis_loss,dis_loss,recon_train,recon_test,recon
    return 0

q3_save_results(q3b, "b")
