import torch
import torch.nn as nn
from torch.optim.lr_scheduler import LambdaLR
from deepul.hw3_helper import *
import deepul.pytorch_util as ptu
import warnings
warnings.filterwarnings('ignore')
device='cuda'

class Depth2Space(nn.Module):
    def __init__(self,block_size:int):
        super().__init__()
        self.block_size=block_size
        self.block_size_sq=block_size*block_size
        
    def forward(self,x:torch.Tensor):
        out=x.permute(0,2,3,1)
        batch_size, d_H,d_W,d_depth=out.shape
        s_depth=int(d_depth/self.block_size_sq)
        s_width=int(d_W*self.block_size)
        s_height=int(d_H*self.block_size)
        t_1=out.reshape((batch_size,d_H,d_W,self.block_size_sq,s_depth))
        spl=t_1.split(self.block_size,3)
        stack=[t_t.reshape(batch_size,d_H,s_width,s_depth) for t_t in spl]
        out=torch.stack(stack,0).transpose(0, 1).permute(0,2,1,3,4).reshape((batch_size,s_height,s_width,s_depth))
        out=out.permute(0,3,1,2)
        return out
    
class Space2Depth(nn.Module):
    def __init__(self,block_size:int):
        super().__init__()
        self.block_size=block_size
        self.block_size_sq=block_size*block_size
    def forward(self,x:torch.Tensor):
        out=x.permute(0,2,3,1)
        batch_size,s_height,s_width,s_depth=out.shape
        d_depth=s_depth*self.block_size_sq
        d_width=int(s_width/self.block_size)
        d_height=int(s_height/self.block_size)
        t_1=out.split(self.block_size,2)
        stack=[ t_t.reshape((batch_size,d_height,d_depth)) for t_t in t_1]
        out=torch.stack(stack,1)
        out=out.permute(0,2,1,3)
        out=out.permute(0,3,1,2)
        return out
class Upsample_Conv2d(nn.Module):
    def __init__(self,in_dim,out_dim,kernel_size=(3,3),stride=1,padding=1):
        super().__init__()
        self.Dep2Sp=Depth2Space(block_size=2)
        self.conv=torch.nn.Conv2d(in_channels=in_dim, out_channels=out_dim, kernel_size=kernel_size,stride=stride,padding=padding)
    def forward(self,x:torch.Tensor):
        x=torch.concat([x,x,x,x],dim=1)
        x=self.Dep2Sp(x)
        x=self.conv(x)
        return x
    
class Downsample_Conv2d(nn.Module):
    def __init__(self,in_dim,out_dim,kernel_size=(3,3),stride=1,padding=1):
        super().__init__()
        self.Sp2Dep=Space2Depth(block_size=2)
        self.conv=torch.nn.Conv2d(in_channels=in_dim, out_channels=out_dim, kernel_size=kernel_size,stride=stride,padding=padding)
    def forward(self,x:torch.Tensor):
        x=self.Sp2Dep(x)
        x1,x2,x3,x4=x.chunk(4,dim=1)
        x=(x1+x2+x3+x4)/4.0
        x=self.conv(x)
        return x
    
class ResnetBlockUp(nn.Module):
    def __init__(self,in_dim:int, kernel_size=(3,3), n_filters=256):
        super().__init__()
        self.bn1=torch.nn.BatchNorm2d(in_dim).to(device)
        self.bn2=torch.nn.BatchNorm2d(n_filters).to(device)
        self.acti=torch.nn.ReLU()
        self.conv=torch.nn.Conv2d(in_channels=in_dim, out_channels=n_filters, kernel_size=kernel_size,padding=1).to(device)
        self.Up1=Upsample_Conv2d(n_filters, n_filters,kernel_size,padding=1).to(device)
        self.Up2=Upsample_Conv2d(in_dim, n_filters,kernel_size=(1,1),padding=0).to(device)
    def forward(self,x:torch.Tensor):
        y=x
        y=self.bn1(y)
        y=self.acti(y)
        y=self.conv(y)
        y=self.bn2(y)
        y=self.acti(y)
        residual=self.Up1(y)
        shortcut=self.Up2(x)
        
        return residual+shortcut
    
class ResnetBlockDown(nn.Module):
    def __init__(self,in_dim:int, kernel_size=(3,3), n_filters=256):
        super().__init__()
        self.acti=torch.nn.ReLU()
        self.conv=torch.nn.Conv2d(in_channels=in_dim, out_channels=n_filters, kernel_size=kernel_size,padding=1).to(device)
        self.D1=Downsample_Conv2d(n_filters, n_filters,kernel_size,padding=1).to(device)
        self.D2=Downsample_Conv2d(in_dim, n_filters,kernel_size=(1,1),padding=0).to(device)
    def forward(self,x:torch.Tensor):
        y=x
        y=self.conv(y)
        y=self.acti(y)
        residual=self.D1(y)
        shortcut=self.D2(x)
        return residual+shortcut
class Generator(nn.Module):
    def __init__(self,n_filters=128):
        super().__init__()
        self.n_filters=n_filters
        self.RU1=ResnetBlockUp(in_dim=256,n_filters=n_filters).to(device)
        self.RU2=ResnetBlockUp(in_dim=n_filters,n_filters=n_filters).to(device)
        self.RU3=ResnetBlockUp(in_dim=n_filters,n_filters=n_filters).to(device)
        self.L=torch.nn.Linear(in_features=128, out_features=256*4*4).to(device)
        self.acti=torch.nn.ReLU()
        self.bn=nn.BatchNorm2d(n_filters).to(device)
        self.tan=nn.Tanh()
        self.conv=nn.Conv2d(n_filters, 3, kernel_size=(3,3),padding=1).to(device)
        
    def sample(self,num_samples=1024):
        
        z=np.random.normal(loc=0,scale=1.0,size=(num_samples,self.n_filters))
        z=torch.Tensor(z).to(device)
        z=self.L(z)
        #reshape
        z=z.reshape((num_samples,256,4,4))
        z=self.RU1(z)
        z=self.RU2(z)
        z=self.RU3(z)
        z=self.bn(z)
        z=self.acti(z)
        z=self.conv(z)
        z=self.tan(z)
        return z
class ResBlock(nn.Module):
    def __init__(self,n_filters=128):
        super().__init__()
        self.conv1=nn.Conv2d(in_channels=n_filters, out_channels=n_filters, kernel_size=(3,3),padding=1).to(device)
        self.acti=nn.ReLU()
        self.conv2=nn.Conv2d(in_channels=n_filters, out_channels=n_filters, kernel_size=(3,3),padding=1).to(device)
    def forward(self,x:torch.Tensor):
        x=self.acti(x)
        x=self.conv1(x)
        x=self.acti(x)
        x=self.conv2(x)
        return x
class Discriminator(nn.Module):
    def __init__(self,n_filters=128):
        super().__init__()
        self.RD1=ResnetBlockDown(3,n_filters=n_filters).to(device)
        self.RD2=ResnetBlockDown(128,n_filters=n_filters).to(device)
        self.Rs1=ResBlock(n_filters)
        self.Rs2=ResBlock(n_filters)
        self.acti=nn.ReLU()
        self.L=nn.Linear(n_filters, 1)
        
    def forward(self,x:torch.Tensor):
        x=self.RD1(x)
        x=self.RD2(x)
        x=self.Rs1(x)
        x=self.Rs2(x)
        x=self.acti(x)
        x=torch.sum(x,dim=[-1,-2])
        x=self.L(x)
        return x

class GAN(nn.Module):
    def __init__(self,n_filters=128):
        super().__init__()
        self.D=Discriminator(n_filters=n_filters).to(device)
        self.G=Generator(n_filters=n_filters).to(device)
    

def q2(train_data):
    total=train_data.shape[0]
    batch_size=256
    print(total)
    n_batches=total//batch_size
    
    model=GAN(n_filters=128)
    model.to(device)
    
    model.load_state_dict(torch.load('q2.pth'))
    opt_D=torch.optim.Adam(model.D.parameters(),lr=2e-4,betas=(0.0,0.9))
    opt_G=torch.optim.Adam(model.G.parameters(),lr=2e-4,betas=(0.0,0.9))
    
    lr_D=lambda step:1-(step/25000)
    sche_D=LambdaLR(opt_D, lr_D)
    
    
    lamb=10
    epoch=5
    train_loss=[1]
    for i in range(0,epoch):
        rand=np.random.randint(0,total,size=(batch_size*n_batches,))
        for j in range(0,n_batches):
            for k in range(0, 5):
                eps=torch.rand(size=(batch_size,1,1,1),device=device)
                eps=eps.broadcast_to((batch_size,3,32,32))
                tilde_x=model.G.sample(batch_size)
                
                indice=np.random.randint(0,batch_size*n_batches,size=(batch_size,))
                rand1=rand[indice]
                x=train_data[rand1]
                x=torch.Tensor(x).to(device)   #(batch_size,3,32,32)
                
                hat_x=eps*x+(1.0-eps)*tilde_x
                
                grad_Dx=torch.autograd.grad(model.D(hat_x),hat_x,grad_outputs=torch.ones(size=(batch_size,1),device=device),
                                            create_graph=True,retain_graph=True)[0]
                
                L=model.D(tilde_x)-model.D(x)+lamb*(torch.norm(grad_Dx,p=2, dim=[1,2,3])-1.0)**2
                L=L.mean()
                opt_D.zero_grad()
                L.backward()
                print(i,j,L.item())
                train_loss.append(L.item())
                opt_D.step()
                sche_D.step()
                
                del eps
                del x,hat_x,tilde_x,grad_Dx
                torch.cuda.empty_cache()
            sample=model.G.sample(batch_size)
            L=-model.D(sample).mean()
            opt_G.zero_grad()
            L.backward()
            opt_G.step()
        torch.save(model.state_dict(),'q2.pth')
    train_loss=np.array(train_loss)
    
    with torch.no_grad():
        sample=model.G.sample(1000).cpu().detach().numpy()
        sample=sample.transpose(0,2,3,1)
        
    
    
    return train_loss,sample 

q2_save_results(q2)
