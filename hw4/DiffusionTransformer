import numpy as np 
import torch
import torch.nn as nn
import einops 
from einops import einsum,rearrange

def get_1d_pos_emb_from_grid(embed_dim,pos):
    assert embed_dim%2==0
    omega=np.arange(embed_dim//2,dtype=np.float64 )
    omega/=embed_dim/2.
    omega=1./10000**omega
    
    pos=pos.reshape(-1)
    out=np.einsum('m,d->md', pos,omega)
    
    sin=np.sin(out)
    cos=np.cos(out)
    
    emb=np.concatenate( (sin,cos),axis=1)
    return emb

def get_2d_pos_emb_from_grid(embed_dim,grid):
    assert embed_dim%2==0
    emb_h=get_1d_pos_emb_from_grid(embed_dim//2, grid[0])
    emb_w=get_1d_pos_emb_from_grid(embed_dim//2, grid[1])
    
    emb=np.concatenate( (emb_h, emb_w),axis=1)
    return emb

def get_2d_pos_emb(embed_dim,grid_size):
    grid_h=np.arange(grid_size, dtype=np.float32)
    grid_w=np.arange(grid_size, dtype=np.float32)
    grid=np.meshgrid(grid_w,grid_h)
    grid=np.stack(grid,axis=0)
    
    grid=grid.reshape([2,1,grid_size,grid_size])
    pos_emb=get_2d_pos_emb_from_grid(embed_dim, grid)
    return pos_emb
    
def modulate(x,shift,scale):
    return x*(1+scale.unsqueeze(1))+shift.unsqueeze(1)

def dot_atten(Q,K,V,mask=None):
    
    QK=einsum(Q,K.transpose(-1,-2)," batch_size ... n d_k, batch_size ... d_k m->batch_size ... n m")
    dk=K.shape[-1]
    QK=QK/np.sqrt(dk)
    if mask!=None:
        out=torch.where(mask==False, -torch.inf,0)
        QK=QK+out
    
    
    func=nn.Softmax(dim=-1)
    out=func(QK)
    out=torch.bmm(out,V)
    return out


class MultiHead_Self_Attention(nn.Module):
    def __init__(self,d_model:int ,num_heads:int):
        super().__init__()
        self.d_model=d_model
        self.h=num_heads
        self.dk=self.d_model//self.h
        self.dv=self.dk
        
        
        
        self.wq=torch.nn.Linear(self.d_model, self.h*self.dk,bias=False)
        self.wk=torch.nn.Linear(self.d_model, self.h*self.dk)
        self.wv=torch.nn.Linear(self.d_model, self.h*self.dv)
        self.wo=torch.nn.Linear(self.h*self.dv, self.d_model)
        self.drop=torch.nn.Dropout(0.1)
        
        
    def forward(self,x:torch.Tensor,q=None,k=None):  # x的最后一维应该和d_model一样
    
        seq_len=x.shape[-2]
        mask=torch.tril(torch.ones(size=(seq_len,seq_len))).bool().to(x.device)
        wq=self.wq(x)
        wk=self.wk(x)
        wv=self.wv(x)
        
        wq=rearrange(wq, "... seq_len (h d_k)->... h seq_len d_k", h=self.h )
        wk=rearrange(wk, "... seq_len (h d_k)->... h seq_len d_k", h=self.h )
        wv=rearrange(wv, "... seq_len (h d_v)->... h seq_len d_v", h=self.h )
        wq=torch.split(wq, split_size_or_sections=1, dim=-3)
        wk=torch.split(wk, split_size_or_sections=1, dim=-3)
        wv=torch.split(wv, split_size_or_sections=1, dim=-3)
        
        
        heads=[ dot_atten(wq[i].squeeze(-3), wk[i].squeeze(-3),wv[i].squeeze(-3),mask=mask) for i in range(0,self.h)]
        
        h=torch.concat(heads,dim=-1)
        
        out=self.wo(h)
        return out

class MLP(nn.Module):
    def __init__(self,d_model,d_ff):
        super().__init__()
        self.d_ff=d_ff
        self.d_model=d_model
        
        self.l1=torch.nn.Linear(d_model, self.d_ff)
        self.act=torch.nn.GELU()
        self.l2=torch.nn.Linear(self.d_ff, self.d_model)
        self.drop=nn.Dropout(0.1)
    def forward(self,x:torch.Tensor):
        x=self.l1(x)
        x=self.act(x)
        x=self.l2(x)
        return self.drop(x)
    

class DiTBlock(nn.Module):
    def __init__(self,hidden_size,num_heads):
        super().__init__()
        self.acti=nn.SiLU()
        self.l1=nn.Linear(hidden_size, 6*hidden_size)
        self.lm1=nn.LayerNorm(hidden_size,elementwise_affine=False)
        
        self.msa=MultiHead_Self_Attention(d_model=hidden_size, num_heads=num_heads)
        
        self.lm2=nn.LayerNorm(hidden_size,elementwise_affine=False)
        self.mlp=MLP(d_model=hidden_size, d_ff=hidden_size)
        
    def forward(self,x:torch.Tensor,c:torch.Tensor):
        # x:(B L D) c:(B D)
        c=self.acti(c)
        c=self.l1(c)
        
        shift_msa,scale_msa,gate_msa,shift_mlp,scale_mlp,gate_mlp=c.chunk(6,dim=1)
        
        h=self.lm1(x)
        h=modulate(h, shift_msa, scale_msa)
        x=x+gate_msa.unsqueeze(1)*self.msa(h)
        
        h=self.lm2(x)
        h=modulate(h, shift_mlp, scale_mlp)
        x=x+gate_mlp.unsqueeze(1)*self.mlp(h)
        
        return x
    
class FinalLayer(nn.Module):
    def __init__(self,hidden_size,patch_size,out_channels):
        super().__init__()
        self.acti=nn.SiLU()
        self.l1=nn.Linear(hidden_size, 2*hidden_size)
        
        self.lm1=nn.LayerNorm(hidden_size,elementwise_affine=False)
        
        self.l2=nn.Linear(hidden_size, patch_size*patch_size*out_channels)
        
    def forward(self,x:torch.Tensor,c:torch.Tensor):
        # x:(B L D) c:(B D)
        c=self.acti(c)
        c=self.l1(c)
        
        shift,scale=c.chunk(2,dim=1)
        x=self.lm1(x)
        x=modulate(x, shift, scale)
        x=self.l2(x)
        
        return x

class Position_Emb(nn.Module):
    def __init__(self,embed_dim:int,grid_size:int):
        super().__init__()
        self.emb=torch.Tensor(get_2d_pos_emb(embed_dim, grid_size))
    def forward(self,x:torch.Tensor):
        
        return x+self.emb.to(x.device)
def timestep_embedding(timesteps,dim,max_period=10000):
    half=dim//2
    freqs=np.exp(-np.log(max_period)*np.arange(0,half,dtype=np.float32 )/half)
    args=timesteps[:,None].astype(np.float32)*freqs[None]
    embedding=np.concatenate( (np.cos(args), np.sin(args)),axis=-1)
    if dim%2==1:
        embedding=np.concatenate((embedding, np.zeros_like(embedding[:,:1])),axis=-1)
    return embedding
class DiT(nn.Module):
    def __init__(self,input_shape,patch_size,hidden_size,num_heads,num_layers,num_classes,droup=0.1):
        super().__init__()
        self.C,self.H,self.W=input_shape
        self.pe=Position_Emb(embed_dim=hidden_size, grid_size=int(np.sqrt(self.C*patch_size*patch_size)))
        self.embedding=torch.nn.Embedding(num_classes+1, hidden_size)
        
        self.l1=torch.nn.Linear(self.C*patch_size*patch_size, hidden_size)
        
        self.patch_size=patch_size
        self.hidden_size=hidden_size
        self.Ditblock=nn.ModuleList()
        self.layers=num_layers
        for i in range(0,num_layers):
            self.Ditblock.append(DiTBlock(hidden_size, num_heads))
        self.final=FinalLayer(hidden_size, patch_size, out_channels=self.C)
        
        self.droup=droup
        self.training=True
        self.num_classes=num_classes
        
    def forward(self,x:torch.Tensor, y:np.ndarray, t:np.ndarray):
        batch_size=y.shape[0]
        x=self.patchfy(x)
        x=self.l1(x)
        x=self.pe(x)
        t=timestep_embedding(t, self.hidden_size)
        t=torch.Tensor(t).to(x.device)
        if self.training:
            nums=max(1,int(batch_size*self.droup))
            ind=torch.randperm(batch_size)[:nums]
            y[ind]=self.num_classes
        y=torch.LongTensor(y).to(x.device)
        
        y=self.embedding(y)
        c=t+y
        
        for i in range(0,self.layers):
            x=self.Ditblock[i](x,c)
            
        x=self.final(x,c)
        x=self.unpatchify(x)
        return x
            
        
        
    def patchfy(self,x:torch.Tensor):
        batch_size=x.shape[0]
        y=x.split(self.H//self.patch_size,dim=-2)
        e=[]
        for item in y:
            for y1 in item.split(self.W//self.patch_size,dim=-1):
                e.append(y1.permute(0,2,3,1).reshape((batch_size,self.H//self.patch_size*self.W//self.patch_size,self.C)))
        
        y=torch.concat(e,dim=-1)
        return y
    
    def unpatchify(self,x:torch.Tensor):
        batch_size=x.shape[0]
        x=x.reshape((batch_size,self.H//self.patch_size,self.W//self.patch_size,self.C*self.patch_size*self.patch_size))
        y=x.split(self.C,dim=-1)
        
        e=[]
        for j in range(0,self.patch_size):
           e.append(torch.concat(y[j*self.patch_size:(j+1)*self.patch_size],dim=-2))
        y=torch.concat(e,dim=-3)
        y=y.permute(0,3,1,2)
        return y
        
        
    def set_training(self):
        self.training=True
    def infer(self):
        self.training=False
        
