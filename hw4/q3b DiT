from deepul.hw4_helper import *
import warnings
warnings.filterwarnings('ignore')

from torch.nn.functional import interpolate
from torch.optim.lr_scheduler import LRScheduler,LinearLR,CosineAnnealingLR,SequentialLR
from DiffusionTransformer import DiT
import numpy as np
import torch
import torch.nn as nn
device='cuda'


class DDPM(nn.Module):
    def __init__(self,input_shape,patch_size,hidden_size,num_heads,num_layers,num_classes,droup=0.1):
        super().__init__()
        self.f=DiT(input_shape, patch_size, hidden_size, num_heads, num_layers, num_classes).to(device)
        self.C,self.H,self.W=input_shape
    def compute_loss(self,eps,label,x_t,t):
       eps=torch.Tensor(eps).to(device)
       x_t=torch.Tensor(x_t).to(device)
       t=torch.Tensor(t).to(device)
       
       
       eps_hat=self.f(x_t,label, t.cpu().detach().numpy())
       L=((eps-eps_hat)**2).mean()
       return L
       
    def sample(self,num_samples,num_steps,label):
        ts=np.linspace(1-1e-4, 1e-4,num_steps+1)
        normal=torch.distributions.MultivariateNormal(loc=torch.zeros(size=(self.C*self.H*self.H,),device=device),
                                                      covariance_matrix=torch.eye(self.C*self.H*self.H,device=device))
        x=normal.sample((num_samples,))
        x=x.reshape((num_samples,self.C,self.H,self.W))
        label=label*np.ones((num_samples,))
        with torch.no_grad():
            for i in range(0,num_steps):
                t=ts[i]*np.ones((num_samples,))
                tm1=ts[i+1]*np.ones((num_samples,))
                t=torch.Tensor(t).to(device)
                tm1=torch.Tensor(tm1).to(device)
                
                eps_hat=self.f(x,label,t.cpu().detach().numpy())
                x=self.update(x, eps_hat, t, tm1)
        return x.cpu().detach().numpy()
            
        
    
    def update(self,x:torch.Tensor,eps_hat,t,tm1):
        with torch.no_grad():
            batch_size=x.shape[0]
            alpha_t=torch.cos(np.pi/2.0*t)[:,np.newaxis,np.newaxis,np.newaxis]
            sigma_t=torch.sin(np.pi/2.0*t)[:,np.newaxis,np.newaxis,np.newaxis]
            
            alpha_tm1=torch.cos(np.pi/2.0*tm1)[:,np.newaxis,np.newaxis,np.newaxis]
            sigma_tm1=torch.sin(np.pi/2.0*tm1)[:,np.newaxis,np.newaxis,np.newaxis]
            
            eta=sigma_tm1/sigma_t*torch.sqrt(1.0-(alpha_t/alpha_tm1)**2)
            
            normal=torch.distributions.MultivariateNormal(loc=torch.zeros(size=(self.C*self.H*self.H,),device=device),
                                                          covariance_matrix=torch.eye(self.C*self.H*self.H,device=device))
            eps_t=normal.sample((batch_size,))
            eps_t=eps_t.reshape((batch_size,self.C,self.H,self.W))
            
            x=alpha_tm1*((x-sigma_t*eps_hat)/alpha_t).clamp(min=-1.0,max=1.0) +torch.sqrt(torch.clamp(sigma_tm1**2-eta**2,min=0.0))*eps_hat+eta*eps_t
        return x


def q3_b(train_data,train_labels,test_data,test_labels,vae):
    
    test_data=test_data.transpose(0,3,1,2)
    test_data=2*(test_data-test_data.min())/(test_data.max()-test_data.min())-1.0
    train_data=train_data.transpose(0,3,1,2)
    train_data=2*(train_data-train_data.min())/(train_data.max()-train_data.min())-1.0
    obj=vae.encode(train_data[0:1000]).cpu().detach().numpy()
    
    
    scale_factor=np.std(obj.flatten())
    print(scale_factor)
    
    train_data=train_data/scale_factor
    test_data=test_data/scale_factor
    
    total,C,H,W=50000,4,8,8
    input_shape=[C,H,W]
    
    batch_size=256
    n_batches=total//batch_size
    
    model=DDPM(input_shape=input_shape, patch_size=2, hidden_size=512, num_heads=8, num_layers=12, num_classes=10)
    model.to(device)
    
    model.load_state_dict(torch.load('q3.pth'))
    
    
    opt=torch.optim.Adam(model.parameters(),lr=1e-4)
    warm_up_sche=LinearLR(opt,start_factor=0.01,total_iters=100,last_epoch=-1)
    cos_sche=CosineAnnealingLR(opt, T_max=800,eta_min=0.0)
    
    sche=SequentialLR(opt, schedulers=[warm_up_sche,cos_sche], milestones=[100])
    
    train_loss=[ 1]
    test_loss=[1 ]
    
    epoch=10
    model.f.set_training()
    for i in range(0,epoch):
        rand=np.random.randint(0,total,(batch_size*n_batches,))
        for j in range(0,n_batches):
            rand1=rand[j*batch_size:(j+1)*batch_size]
            data=train_data[rand1]
            x=vae.encode(data).cpu().detach().numpy()
            t=np.random.uniform(0,1,size=(batch_size,))
            alpha=np.cos(np.pi/2.0*t)
            sigma=np.sin(np.pi/2.0*t)
            eps=np.random.multivariate_normal(mean=np.zeros(C*H*W,), cov=np.eye(C*H*W),size=(batch_size,))
            eps=eps.reshape((x.shape[0],C,H,W))
            x_t=alpha[:,np.newaxis,np.newaxis,np.newaxis]*x+sigma[:,np.newaxis,np.newaxis,np.newaxis]*eps
            
            label=train_labels[rand1]
            
            L=model.compute_loss(eps, label, x_t, t)
            print(i,L.item())
            train_loss.append(L.item())
            opt.zero_grad()
            L.backward()
            opt.step()
            sche.step()
        
        torch.save(model.state_dict(),'q3.pth')
        with torch.no_grad():
            data=test_data[:50]
            x=vae.encode(data).cpu().detach().numpy()
            t=np.random.uniform(0,1,size=(50,))
            alpha=np.cos(np.pi/2.0*t)
            sigma=np.sin(np.pi/2.0*t)
            eps=np.random.multivariate_normal(mean=np.zeros(C*H*W,), cov=np.eye(C*H*W),size=(50,))
            eps=eps.reshape((x.shape[0],C,H,W))
            x_t=alpha[:,np.newaxis,np.newaxis,np.newaxis]*x+sigma[:,np.newaxis,np.newaxis,np.newaxis]*eps
            
            label=test_labels[:50]
            
            L=model.compute_loss(eps, label, x_t, t)
            test_loss.append(L.item())
    
    model.f.infer()
    sample=[]
    for i in range(0,10):
        s=model.sample(10, 512, label=i)
        s=scale_factor*s
        
        sample.append(vae.decode(s).cpu().detach().numpy())
    sample=np.stack(sample,axis=0)
    sample=sample.transpose(0,1,3,4,2)
    sample=(sample-sample.min())/(sample.max()-sample.min())
    
    
    return train_loss,test_loss,sample

q3b_save_results(q3_b)
