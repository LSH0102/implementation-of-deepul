import copy
import torch
import numpy as np
from torch import nn
import einops 
from einops import einsum,rearrange
device='cuda'
class Linear(nn.Module):
    def __init__(self,in_features, out_features,device=None,dtype=None):
        super().__init__()
        t=torch.empty(out_features,in_features)
        sigma=np.sqrt(2/(in_features+out_features))
        self.W=nn.Parameter(torch.nn.init.trunc_normal_(t,mean=0,std=sigma,a=-3*sigma,b=3*sigma))
        
    def forward(self, x:torch.Tensor):
        return torch.matmul(x, self.W.T)
    
class Embedding(nn.Module):
    def __init__(self,num_embeddings,embedding_dim,device=device,dtype=None):
        super().__init__()
        t=torch.empty(num_embeddings,embedding_dim)
        self.W=nn.Parameter(torch.nn.init.trunc_normal_(t,mean=0,std=1,a=-3,b=3))
        self.d_model=embedding_dim
    def forward(self, x:torch.LongTensor):
        seq_len=x.shape[1]
        batch_size=x.shape[0]
        y=x.flatten().unsqueeze(1)
        y=y.broadcast_to((y.shape[0],self.d_model)).to(torch.int64).to(device)
        
        z=torch.gather(self.W, dim=0, index=y)
        
        z=z.reshape((batch_size,seq_len,self.d_model))
        return z
    
class RMSNorm(nn.Module):
    def __init__(self, d_model: int, eps: float = 1e-5, device=device, dtype=None):
        super().__init__()
        self.g=nn.Parameter(torch.ones((d_model,))).to(device)
        self.d_model=d_model
        self.eps=eps
    def forward(self,x:torch.Tensor):
        in_type=x.dtype
        x=x.to(torch.float32)
        
        rms=torch.sqrt(torch.sum(x**2,dim=-1,keepdim=True)/self.d_model+self.eps)
        x=x/rms
        x=x*self.g
        
        return x.to(in_type)
    
class SiLU(nn.Module):
    def __init__(self,):
        super().__init__()
        self.sigmoid=torch.nn.Sigmoid()
    def forward(self,x:torch.Tensor):
        return x*self.sigmoid(x)
        
class SwiGLU(nn.Module):
    def __init__(self,d_ff,d_model,device=device,dtype=None):
        super().__init__()
        self.d_ff=d_ff
        self.d_model=d_model
        
        t1=torch.empty(self.d_ff,self.d_model)
        t3=torch.empty(self.d_ff,self.d_model)
        t2=torch.empty(self.d_model,self.d_ff)
        sigma=np.sqrt(2/(self.d_ff+self.d_model))
        
        self.W1=nn.Parameter(torch.nn.init.trunc_normal_(t1,mean=0,std=sigma,a=-3*sigma,b=3*sigma)).to(device)
        self.W2=nn.Parameter(torch.nn.init.trunc_normal_(t2,mean=0,std=sigma,a=-3*sigma,b=3*sigma)).to(device)
        self.W3=nn.Parameter(torch.nn.init.trunc_normal_(t3,mean=0,std=sigma,a=-3*sigma,b=3*sigma)).to(device)
        
        self.sigmoid=torch.nn.GELU()
        
    def forward(self,x:torch.Tensor):
        a=torch.matmul(x, self.W1.T)
        a=self.sigmoid(a)
        
        c=torch.matmul(x, self.W3.T)
        a=a*c
        
        return torch.matmul(a, self.W2.T)
    
class RoPE(nn.Module):
    def __init__(self,theta:float, d_k:int,max_seq_len:int,device=device):
        super().__init__()
        self.d=d_k
        self.len=max_seq_len
        self.k=self.d//2 
        self.theta=theta
        
        vec=np.array([1/theta**(2*k/self.d) for k in range(0,self.k)])
        l=np.arange(self.len)
        self.grid=np.einsum('m,d->md',l,vec) #max_len x d/2
        self.sin=np.sin(self.grid)
        self.cos=np.cos(self.grid)
        
        L1=np.stack([self.cos,-self.sin],axis=-1)
        L2=np.stack([self.sin,self.cos],axis=-1)
        rotation=torch.Tensor(np.stack([L1,L2],axis=-2))
        self.rope=torch.stack([torch.block_diag(*rotation[i]) for i in range(0,self.len)],dim=0).to(device)
        
    def forward(self,x:torch.Tensor, token_positions:torch.Tensor):
        token_positions=token_positions.reshape((-1,1,1))
        y=token_positions.broadcast_to(size=(token_positions.shape[0],self.d,self.d))
        
        rot=torch.gather(self.rope, dim=0, index=y)
        
        out=einsum( x,rot,"... seq_len d_1, seq_len d_2 d_1->... seq_len d_2")
        return out

class ABS_RoPE(nn.Module):
    def __init__(self,theta:float, d_k:int, max_seq_len:int,device=device):
        super().__init__()
        self.d=d_k
        self.len=max_seq_len
        self.k=self.d//2 
        self.theta=theta
        vec=np.array([1/theta**(2*k/self.d) for k in range(0,self.k)])
        l=np.arange(self.len)
        self.grid=np.einsum('m,d->md',l,vec) #max_len x d/2
        self.sin=np.sin(self.grid)
        self.cos=np.cos(self.grid)
        s=np.stack([self.sin,self.cos],axis=1)
        
        self.rope=torch.Tensor(s.transpose(0,-1,-2).reshape((self.len),-1)).to(device)
        
        
    def forward(self,x:torch.Tensor):
        seq_len=x.shape[1]
        pe=self.rope[:seq_len,:]
        return x+pe

class Softmax(nn.Module):
    def __init__(self,dim:int):
        super().__init__()
        self.dim=dim
        
    def forward(self,x:torch.Tensor):
        max_x,_=torch.max(x,dim=self.dim,keepdim=True)
        x=x-max_x
        x=torch.exp(x)
        y=x/x.sum(dim=self.dim,keepdim=True)
        return y
    
def dot_atten(Q,K,V,mask=None):
    
    QK=einsum(Q,K.transpose(-1,-2)," batch_size ... n d_k, batch_size ... d_k m->batch_size ... n m")
    dk=K.shape[-1]
    QK=QK/np.sqrt(dk)
    if mask!=None:
        out=torch.where(mask==False, -torch.inf,0)
        QK=QK+out
    
    
    func=Softmax(-1)
    out=func(QK)
    out=torch.bmm(out,V)
    return out


class MultiHead_Self_Attention(nn.Module):
    def __init__(self,d_model:int ,num_heads:int, device=device):
        super().__init__()
        self.d_model=d_model
        self.h=num_heads
        self.dk=self.d_model//self.h
        self.dv=self.dk
        
        
        
        
        self.wq=torch.nn.Linear(self.d_model, self.h*self.dk,bias=False)
        self.wk=torch.nn.Linear(self.d_model, self.h*self.dk)
        self.wv=torch.nn.Linear(self.d_model, self.h*self.dv)
        self.wo=torch.nn.Linear(self.h*self.dv, self.d_model)
        self.drop=torch.nn.Dropout(0.1)
        
        
    def forward(self,x:torch.Tensor,q=None,k=None):  # x的最后一维应该和d_model一样
    
        seq_len=x.shape[-2]
        mask=torch.tril(torch.ones(size=(seq_len,seq_len))).bool().to(device)
        wq=self.wq(x)
        wk=self.wk(x)
        wv=self.wv(x)
        
        wq=rearrange(wq, "... seq_len (h d_k)->... h seq_len d_k", h=self.h )
        wk=rearrange(wk, "... seq_len (h d_k)->... h seq_len d_k", h=self.h )
        wv=rearrange(wv, "... seq_len (h d_v)->... h seq_len d_v", h=self.h )
        wq=torch.split(wq, split_size_or_sections=1, dim=-3)
        wk=torch.split(wk, split_size_or_sections=1, dim=-3)
        wv=torch.split(wv, split_size_or_sections=1, dim=-3)
        
        
        heads=[ dot_atten(wq[i].squeeze(-3), wk[i].squeeze(-3),wv[i].squeeze(-3),mask=mask) for i in range(0,self.h)]
        
        h=torch.concat(heads,dim=-1)
        
        out=self.wo(h)
        return out
        
class Transformer_Block(nn.Module):
    def __init__(self,d_model:int,num_heads:int,d_ff:int,device=device):
        super().__init__()
        
        
        self.MultiHead1=MultiHead_Self_Attention(d_model, num_heads,device=device)
        
        self.FFN=FFN(d_ff=d_ff, d_model=d_model,device=device)
        
        self.Layer1=torch.nn.LayerNorm(d_model,bias=True)
        self.Layer2=torch.nn.LayerNorm(d_model,bias=True)
    def forward(self,x:torch.Tensor):
        x=self.Layer1(x+self.MultiHead1(x))
        
        
        x=self.Layer2(x+self.FFN(x))
        return x
        

class FFN(nn.Module):
    def __init__(self,d_model,d_ff,device=device):
        super().__init__()
        self.d_ff=d_ff
        self.d_model=d_model
        
        self.l1=torch.nn.Linear(d_model, self.d_ff)
        self.act=torch.nn.GELU()
        self.l2=torch.nn.Linear(self.d_ff, self.d_model)
        self.drop=torch.nn.Dropout(0.1).to(device)
    def forward(self,x:torch.Tensor):
        x=self.l1(x)
        x=self.act(x)
        x=self.l2(x)
        return self.drop(x)
    
        
        

    
def _get_clones(module, N):
    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])

class Transformer(nn.Module):
    def __init__(self,vocab_size:int, context_length:int, num_layers:int, d_model:int,
                 num_heads:int,d_ff:int,theta=None,max_seq_len=None):
        super().__init__()
        
        if max_seq_len==None:
            max_seq_len=context_length
        
        self.pe=ABS_RoPE(theta, d_model, max_seq_len)
        self.embedding=Embedding(vocab_size, d_model).to(device)
        
        Layer=Transformer_Block(d_model, num_heads, d_ff)
        
        self.Trans=_get_clones(Layer,num_layers)
        
        
        self.Linear=torch.nn.Linear(d_model, vocab_size)
        
        self.lm=torch.nn.LayerNorm(d_model).to(device)
        self.layers=num_layers
        self.d_model=d_model
    def forward(self,x:torch.Tensor):
        x=self.embedding(x)
        x=self.pe(x)
        for i in range(0,self.layers):
            x=self.Trans[i](x)
            
        x=self.lm(x)
        x=self.Linear(x)
        
        
        return x
